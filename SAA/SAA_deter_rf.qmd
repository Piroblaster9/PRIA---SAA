---
title: "Unidad 3(Random Forest): Determinación de Sistemas de Aprendizaje automático"
subtitle: "Random Forest"
autor: "Jesús Turpín"
format:
  html:
    code-fold: true
editor: visual
toc: true
toc-depth: 3
bibliography: SAA_U3_refs.bib
---

```{r warning=FALSE, message=FALSE}
library(tidyverse)
# library(rpart)
# library(rpart.plot)
# library(bayesQR)
library(broom)
library(corrplot)
# library(yardstick)
library(pROC)
# library(ggrepel)
# library(purrr)
library(plotly)
library(randomForest)
library(skimr)
library(MASS)
```

## Algoritmos de Machine Learning

-   Regresión Lineal

-   Regresión Logística

-   Árboles de Decisión

-   **Random Forest**

-   k Nearest Neighbor (KNN)

-   SVM

-   Naive Bayes

-   Clustering jerárquico

-   K-Means

-   PCA

-   Redes Neuronales

-   Aprendizaje profundo

## Bibliografía

-   @aprende_ml_espanol

-   @FAVERO2023259

-   @JoaquinAR2024RPubs

## Aproximación a la idea de "Bosque aleatorio"

Imagínate que tienes un grupo de amigos que te ayudan a tomar decisiones. Cada uno de ellos te da su opinión basada en sus experiencias y conocimientos. Al final, decides basándote en la mayoría de sus opiniones. Random Forest funciona de manera similar, pero en lugar de amigos, tienes un conjunto de árboles de decisión.

Cada árbol se entrena con una muestra aleatoria de los datos (con reemplazo) y al hacer una predicción, cada árbol vota. La predicción final es el resultado de la mayoría de los votos de todos los árboles. Esta técnica mejora la precisión de la predicción y controla el sobreajuste, que es un problema común cuando se usa un solo árbol de decisión.

### Ensemble

Las técnicas de *ensemble* combinan múltiples modelos en uno nuevo con el objetivo equilibrar sesgo vs varianza, consiguiendo así mejores predicciones que cualquiera de los modelos individuales originales. Dos de los tipos de ensemble más utilizados son:

**Bagging**: Se ajustan múltiples modelos, cada uno con un subconjunto distinto de los datos de entrenamiento. Para predecir, todos los modelos que forman el agregado participan aportando su predicción. Como valor final, se toma la media de todas las predicciones (variables continuas) o la clase más frecuente (variables categóricas). **Random Forest está dentro de esta categoría**.

**Boosting**: Se ajustan secuencialmente múltiples modelos sencillos, llamados weak learners, de forma que cada modelo aprende de los errores del anterior. Como valor final, al igual que en bagging, se toma la media de todas las predicciones (variables continuas) o la clase más frecuente (variables cualitativas). Tres de los métodos de boosting más empleados son AdaBoost, Gradient Boosting y Stochastic Gradient Boosting. Veremos alguno de ellos en la unidad siguiente.

## Fundamentos de Random Forest

Random Forest es un algoritmo de aprendizaje *ensemble*, lo que significa que combina las predicciones de varios modelos (en este caso, árboles de decisión) para generar una salida más precisa y robusta. Se basa en dos conceptos clave del ML: el **bagging** (bootstrap aggregating) y la **selección aleatoria de variables predictivas**.

Dado que su origen son los árboles de decisión, se utiliza tanto para problemas de clasificación como de regresión. En clasificación, el resultado se obtiene con la moda de la clase en el conjunto de árboles o en probabilidades como la media de las probabilidades de cada clase del conjunto de árboles. En regresión se usa la media obtenida en cada árbol.

### Bagging

Este método mejora la estabilidad y precisión de los algoritmos de machine learning. Consiste en generar múltiples conjuntos de datos de entrenamiento mediante muestreo aleatorio con reemplazo (bootstrap), entrenar un modelo en cada uno de estos conjuntos y luego combinar sus predicciones. En el caso de Random Forest, cada conjunto de datos de entrenamiento se usa para entrenar un árbol de decisión diferente.

### Selección aleatoria de predictores

Al construir cada árbol, en cada división o nodo, en lugar de buscar la mejor característica entre todas las posibles para dividir el conjunto de datos, Random Forest selecciona un subconjunto aleatorio de características y busca la mejor entre este subconjunto. Esto añade diversidad a los modelos, aumentando la robustez del conjunto total y haciéndolo menos sensible a outliers.

Antes de cada división, se seleccionan aleatoriamente m predictores de un total de p. La diferencia en el resultado dependerá del valor m escogido. Si m=p, los resultados de random forest y bagging son equivalentes.

Algunas recomendaciones son:

-   La raíz cuadrada del número total de predictores para problemas de clasificación. $m \approx \sqrt{p}$

-   Un tercio del número de predictores para problemas de regresión. $m\approx \frac{p}3$

-   Si los predictores están muy correlacionados, valores pequeños de m consiguen mejores resultados.

Aumentar el número de árboles, no produce overfit y mejora el modelo hasta cierto número en el que se estabiliza. No tiene sentido seguir aumentando, pues se debe buscar reducir el coste computacional.

## Paquete randomForest e Hiperparámetros

```{r}
# install.packages("randomForest)
# install.packages("ranger")
```

En R, el paquete que tradicionalmente se ha usado para este algoritmo es `randomForest`. En la documentación de la función con el mismo nombre podemos ver que algunos de los atributos son hiperparámetros.

En la actualidad, el paquete `ranger`, mejora la implementación original e incluye más opciones de configuración.

-   `ntree`: Número de árboles (n_estimators). Más árboles aumentan la precisión pero también el costo computacional. Por defecto, suele estar configurado en 500

-   Profundidad máxima del árbol (max_depth): La profundidad máxima de cada árbol. Limitar la profundidad puede ayudar a prevenir el sobreajuste. `maxnodes`

-   `mtry`: Número máximo de variables predictoras (max_features). El número de características a considerar cuando se busca la mejor división.

-   `nodesize`: Mínimo número de muestras para dividir (min_samples_split): El número mínimo de muestras que debe tener un nodo antes de que pueda ser dividido.

## Regresión con Random Forest

```{r}
data("Boston")
```

```{r}
glimpse(Boston)
```

```{r}
set.seed(131)
boston.rf <- randomForest(medv ~ ., data=Boston)
print(boston.rf)
```

```{r}
residuos <- Boston$medv - boston.rf$predicted
```

```{r}
MSE <- sum(residuos**2)/(nrow(Boston))
RMSE <- sqrt(MSE)
RMSE
```

## Clasificación con Random Forest

```{r}
bank <- read.csv("data/bank.csv")
```

```{r}
glimpse(bank)
```

```{r}
bank_pp <- bank %>%
  mutate_if(is.character, as.factor) 
```

```{r}
glimpse(bank_pp)
```

```{r}
set.seed(123)
bank.rfmdl <- randomForest(deposit ~ ., data = bank_pp, ntree = 250 )
```

```{r}
print(bank.rfmdl)
```

```{r}
deposits <- bank_pp$deposit == "yes"
```

```{r}
predicted_probs <- predict(bank.rfmdl, data = bank_pp, type = "prob")
predicted_probs <- predicted_probs[,2]
```

```{r warning=FALSE, message=FALSE}
ROC <- roc(deposits, predicted_probs)
plot(ROC, col = "blue")    
text(0.5, 0.2, paste("AUC =", round(auc(ROC), 3)))
       
```

## Ejercicios

1.- Repite los ejercicios de regresión y clasificación usando el paquete `ranger` y/o `h2o`. Busca información y explica las diferencias principales en los flujos de trabajo (funciones y parámetros).

```{r}
#USANDO RANGER

# Instalar y cargar el paquete
install.packages("ranger")
library(ranger)

# Regresión con ranger
set.seed(131)
boston.rf_ranger <- ranger(medv ~ ., data = Boston)
print(boston.rf_ranger)

# Clasificación con ranger
set.seed(123)
bank.rfmdl_ranger <- ranger(deposit ~ ., data = bank_pp, num.trees = 250)
print(bank.rfmdl_ranger)

```

```{r}
# USANDO H2O
# Instalar y cargar el paquete
install.packages("h2o")
library(h2o)

# Inicializar y conectar un cluster de H2O
h2o.init()

# Convertir los datos al formato de H2O
boston_h2o <- as.h2o(Boston)
bank_h2o <- as.h2o(bank_pp)

# Regresión con H2O
boston.rf_h2o <- h2o.randomForest(training_frame = boston_h2o, x = names(boston_h2o)[!names(boston_h2o) %in% "medv"], y = "medv")
print(boston.rf_h2o)

# Clasificación con H2O
bank.rfmdl_h2o <- h2o.randomForest(training_frame = bank_h2o, x = names(bank_h2o)[!names(bank_h2o) %in% "deposit"], y = "deposit", ntrees = 250)
print(bank.rfmdl_h2o)

#Investigación de diferencias:
#Sintaxis de las funciones:

#En randomForest, utilizamos la función randomForest() para ajustar el modelo tanto para regresión como para clasificación.
#En ranger, usamos la función ranger() para ajustar el modelo tanto para regresión como para clasificación.
#En h2o, utilizamos funciones específicas para cada tipo de modelo, como h2o.randomForest() para Random Forest.
#Parámetros disponibles:

#Si bien muchos de los parámetros son similares entre los tres paquetes (como el número de árboles ntree o num.trees), existen diferencias en la forma en que se especifican o se nombran algunos parámetros. Por ejemplo, en ranger el parámetro para el número de árboles se llama num.trees, mientras que en randomForest es ntree.
#Además, ranger y h2o pueden tener más parámetros disponibles que randomForest, lo que permite un mayor control sobre el proceso de ajuste del modelo y la optimización.
#Velocidad de entrenamiento y predicción:

#ranger y h2o suelen ser más rápidos que randomForest, especialmente cuando se trata de conjuntos de datos grandes. Esto se debe a las implementaciones optimizadas y a la capacidad de paralelización que ofrecen estos paquetes.
#Escalabilidad:

#Tanto ranger como h2o están diseñados para ser altamente escalables, lo que significa que pueden manejar conjuntos de datos grandes y complejos de manera más eficiente que randomForest. Esto los hace ideales para aplicaciones que requieren el análisis de grandes volúmenes de datos.

```

2.- Dado el siguiente código:

```{r eval=FALSE}
num_trees <- seq(50, 600, by=50) # Evaluar modelos con diferentes números de árboles
oob_error <- numeric(length(num_trees)) # Para almacenar el error OOB de cada modelo

# Bucle para entrenar modelos y recoger errores OOB
for (i in seq_along(num_trees)) {
  set.seed(123) # Para reproducibilidad
  rf_model <- randomForest(deposit ~ ., data=bank_pp, ntree=num_trees[i], mtry=4, keep.forest=FALSE)
  # Acceder al último valor del error OOB
  oob_error[i] <- rf_model$err.rate[length(rf_model$err.rate)]
}

# Creación de un dataframe para los resultados
results <- data.frame(num_trees, oob_error)

# Gráfico de cómo converge el error OOB con diferentes números de árboles
ggplot(results, aes(x=num_trees, y=oob_error)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(title="Convergencia del Error OOB en función del Número de Árboles",
       x="Número de Árboles",
       y="Error OOB")

##############################################################################


# Cargar paquetes necesarios
library(randomForest)
library(ggplot2)

# Definir el rango de números de árboles a evaluar
num_trees <- seq(50, 600, by = 50)

# Inicializar vector para almacenar el error OOB de cada modelo
oob_error <- numeric(length(num_trees))

# Bucle para entrenar modelos y recoger errores OOB
for (i in seq_along(num_trees)) {
  set.seed(123) # Para reproducibilidad
  rf_model <- randomForest(medv ~ ., data = Boston, ntree = num_trees[i], mtry = 4, keep.forest = FALSE)
  # Acceder al último valor del error OOB
  oob_error[i] <- rf_model$err.rate[length(rf_model$err.rate)]
}

# Crear un dataframe para los resultados
results <- data.frame(num_trees, oob_error)

# Gráfico de cómo converge el error OOB con diferentes números de árboles
ggplot(results, aes(x = num_trees, y = oob_error)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(title = "Convergencia del Error OOB en función del Número de Árboles",
       x = "Número de Árboles",
       y = "Error OOB")

```

Encuentra el número óptimo de árboles a partir del cual los resultados convergen y no se mejora por incrementar el número de árboles. Repite el código para el algoritmo de regresión.

3.- Con la ayuda de chatGPT, intenta optimizar el parámetro ntree, usando el paquete `ranger` y/o `h2o`. ¿Mejora el rendimiento?

```{r}
#USANDO RANGER
# Instalar y cargar el paquete
install.packages("ranger")
library(ranger)

# Definir la cuadrícula de parámetros
ntree_grid <- seq(50, 600, by = 50)

# Inicializar vector para almacenar los errores OOB de cada modelo
oob_errors_ranger <- numeric(length(ntree_grid))

# Bucle para entrenar modelos y recoger errores OOB
for (i in seq_along(ntree_grid)) {
  set.seed(123) # Para reproducibilidad
  rf_model_ranger <- ranger(deposit ~ ., data = bank_pp, num.trees = ntree_grid[i])
  oob_errors_ranger[i] <- rf_model_ranger$prediction.error
}

# Encontrar el mejor valor de ntree
best_ntree_ranger <- ntree_grid[which.min(oob_errors_ranger)]

# Imprimir el mejor valor de ntree
print(paste("El mejor valor de ntree encontrado con ranger es:", best_ntree_ranger))

```

```{r}
#USANDO H2O
# Instalar y cargar el paquete
install.packages("h2o")
library(h2o)

# Inicializar y conectar un cluster de H2O
h2o.init()

# Convertir los datos al formato de H2O
bank_h2o <- as.h2o(bank_pp)

# Definir la cuadrícula de parámetros
ntree_grid <- seq(50, 600, by = 50)

# Inicializar vector para almacenar los errores OOB de cada modelo
oob_errors_h2o <- numeric(length(ntree_grid))

# Bucle para entrenar modelos y recoger errores OOB
for (i in seq_along(ntree_grid)) {
  rf_model_h2o <- h2o.randomForest(training_frame = bank_h2o, x = names(bank_h2o)[!names(bank_h2o) %in% "deposit"], y = "deposit", ntrees = ntree_grid[i])
  oob_errors_h2o[i] <- rf_model_h2o@model$validation_frame$mean_per_class_error[1]
}

# Encontrar el mejor valor de ntree
best_ntree_h2o <- ntree_grid[which.min(oob_errors_h2o)]

# Imprimir el mejor valor de ntree
print(paste("El mejor valor de ntree encontrado con h2o es:", best_ntree_h2o))

```
