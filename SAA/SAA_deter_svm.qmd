---
title: "Unidad 3(Random Forest): Determinación de Sistemas de Aprendizaje automático"
subtitle: "Random Forest"
autor: "Jesús Turpín"
format:
  html:
    code-fold: true
editor: visual
toc: true
toc-depth: 3
bibliography: SAA_U3_refs.bib
---

```{r warning=FALSE, message=FALSE}
library(kknn)

# library(rpart)
# library(rpart.plot)
# library(bayesQR)
library(broom)
library(corrplot)
# library(yardstick)
library(pROC)
# library(ggrepel)
# library(purrr)
library(plotly)
library(randomForest)
library(skimr)
library(MASS)
library(tidyverse)
library(e1071)
```

## Algoritmos de Machine Learning

-   Regresión Lineal

-   Regresión Logística

-   Árboles de Decisión

-   Random Forest

-   k Nearest Neighbor (KNN)

-   **SVM**

-   Naive Bayes

-   Clustering jerárquico

-   K-Means

-   PCA

-   Redes Neuronales

-   Aprendizaje profundo

# Algoritmo SVM para Clasificación y Regresión

@JoaquinAR2024RPubssvm

Las Máquinas de Soporte Vectorial (SVM) son una técnica de aprendizaje supervisado utilizada tanto para clasificación (SVC) como para regresión (SVR). En su forma más básica, el objetivo de SVM es encontrar un hiperplano en un espacio N-dimensional (N --- el número de características) que clasifique claramente los puntos de datos.

## SVM para Clasificación

En clasificación, SVM busca el hiperplano que separa mejor las clases con el margen más amplio. Este hiperplano se define de tal manera que la distancia entre el hiperplano y el punto de datos más cercano de cada clase (vectores de soporte) es maximizada. Matemáticamente, si tenemos un conjunto de entrenamiento de puntos de datos $x_i$ con etiquetas $y_i$ donde $y_i \in \{1, -1\}$, el hiperplano se define como los puntos $x$ que satisfacen la ecuación $w \cdot x - b = 0$. Aquí, $w$ es el vector normal al hiperplano, y $b$ es el sesgo.

## SVM para Regresión

En regresión (SVR), el objetivo es encontrar un hiperplano que se ajuste lo mejor posible a los datos dentro de un margen ε, pero al mismo tiempo sea lo más plano posible. Esto se logra a través de un enfoque similar al de la clasificación, pero en este caso, el hiperplano se diseña para encajar dentro de un tubo ε que captura la mayoría de los puntos.

## Preprocesamiento de Datos

Es importante escalar las variables predictoras antes de aplicar SVM, ya que el algoritmo es sensible a las escalas de las características. Esto asegura que todas las características contribuyan equitativamente al modelo.

## Ejemplo en R

### Clasificación con SVM

```{r}
load("data/corruption.RData")
glimpse(corruption)
```

# Algoritmo SVM para Clasificación y Regresión

Las Máquinas de Soporte Vectorial (SVM) son una técnica de aprendizaje supervisado utilizada tanto para clasificación (SVC) como para regresión (SVR). En su forma más básica, el objetivo de SVM es encontrar un hiperplano en un espacio N-dimensional (N --- el número de características) que clasifique claramente los puntos de datos.

## SVM para Clasificación

En clasificación, SVM busca el hiperplano que separa mejor las clases con el margen más amplio. Este hiperplano se define de tal manera que la distancia entre el hiperplano y el punto de datos más cercano de cada clase (vectores de soporte) es maximizada. Matemáticamente, si tenemos un conjunto de entrenamiento de puntos de datos $x_i$ con etiquetas $y_i$ donde $y_i \in \{1, -1\}$, el hiperplano se define como los puntos $x$ que satisfacen la ecuación $w \cdot x - b = 0$. Aquí, $w$ es el vector normal al hiperplano, y $b$ es el sesgo.

## SVM para Regresión

En regresión (SVR), el objetivo es encontrar un hiperplano que se ajuste lo mejor posible a los datos dentro de un margen ε, pero al mismo tiempo sea lo más plano posible. Esto se logra a través de un enfoque similar al de la clasificación, pero en este caso, el hiperplano se diseña para encajar dentro de un tubo ε que captura la mayoría de los puntos.

## Preprocesamiento de Datos

Es importante escalar las variables predictoras antes de aplicar SVM, ya que el algoritmo es sensible a las escalas de las características. Esto asegura que todas las características contribuyan equitativamente al modelo.

## Ventajas de SVM

-   Alto rendimiento en espacios de alta dimensión.
-   Efectivo en casos donde el número de dimensiones supera el número de muestras.
-   Versátil, gracias a la función del kernel.

## Desventajas de SVM

-   Selección del kernel y sus parámetros puede ser complicado.
-   No es adecuado para grandes conjuntos de datos debido a su alta complejidad computacional.
-   Resultados difíciles de interpretar en comparación con algunos modelos más simples.

## Comparación con Otros Modelos

-   CART y RandomForest: Más interpretables y pueden manejar mejor los datos no lineales y las interacciones.
-   KNN: SVM es más eficaz en espacios de alta dimensión pero KNN es más simple y fácil de implementar.
-   Modelos de regresión logística: Menos complejos y ofrecen resultados más interpretables, pero SVM puede manejar mejor los espacios de características complejos y no lineales.

## Ejemplo en R

### Clasificación con SVM

```{r}
# Preprocesamiento
load("data/corruption.RData")
corruption <- corruption %>%
  dplyr::select(-country) %>%
  # escalado
  mutate_if(is.numeric, scale) 
```

```{r}
model_svc <- svm(emergent_country ~ ., data = corruption)
```

```{r}
predictions_svc <- predict(model_svc, corruption)
```

## Ejercicios

1.  Completar el código para dibujar la curva ROC, mostrar matriz de confusión y resto de métricas

    ```{r}
    #  USANDO SVM

    # Calcular probabilidades de clase
    probabilidades_svc <- predict(model_svc, corruption, probability = TRUE)

    # Calcular la curva
    ROC_svc <- roc(ifelse(corruption$emergent_country == "yes", 1, 0), probabilidades_svc[, 2])

    # Dibujar la curva
    plot(ROC_svc, col = "blue")

    # Calcular el AUC
    auc_svc <- auc(ROC_svc)
    print(paste("El área bajo la curva (AUC) es:", auc_svc))

    # Matriz de confusión
    confusion_matrix_svc <- table(true = ifelse(corruption$emergent_country == "yes", 1, 0), predicted = predictions_svc)

    print("Matriz de Confusión:")
    print(confusion_matrix_svc)

    # Calcular métricas
    accuracy_svc <- sum(diag(confusion_matrix_svc)) / sum(confusion_matrix_svc)
    precision_svc <- confusion_matrix_svc[2, 2] / sum(confusion_matrix_svc[, 2])
    recall_svc <- confusion_matrix_svc[2, 2] / sum(confusion_matrix_svc[2, ])
    f1_score_svc <- 2 * precision_svc * recall_svc / (precision_svc + recall_svc)

    print(paste("Exactitud (Accuracy):", accuracy_svc))
    print(paste("Precisión (Precision):", precision_svc))
    print(paste("Recuperación (Recall):", recall_svc))
    print(paste("Puntuación F1 (F1 Score):", f1_score_svc))

    ```

2.  Implementar el ejercicio para el caso de regresión, usando como variable objetivo cpi

```{r}
# REGRESIÓN CON SVM

if (!exists("corruption")) {
  load("data/corruption.RData")
}

# Preprocesar
corruption <- corruption %>%
  dplyr::select(-country) %>%
  # Escalado
  mutate_if(is.numeric, scale) 

# Dividir los datos en conjunto de entrenamiento y prueba
set.seed(123)
indices <- sample(nrow(corruption), nrow(corruption) * 0.8)
train_data <- corruption[indices, ]
test_data <- corruption[-indices, ]

# Entrenar el modelo 
model_svr <- svm(cpi ~ ., data = train_data)

# predicciones
predictions_svr <- predict(model_svr, test_data)

# Calcular el RMSE
rmse_svr <- sqrt(mean((predictions_svr - test_data$cpi)^2))
print(paste("El error cuadrático medio (RMSE) es:", rmse_svr))

# Calcular R^2
r_squared_svr <- 1 - sum((test_data$cpi - predictions_svr)^2) / sum((test_data$cpi - mean(test_data$cpi))^2)
print(paste("El coeficiente de determinación (R^2) es:", r_squared_svr))

```
